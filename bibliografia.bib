
@misc{guervos_jjplantilla-tfg-etsiit_2024,
	title = {{JJ}/plantilla-{TFG}-{ETSIIT}},
	copyright = {GPL-3.0},
	url = {https://github.com/JJ/plantilla-TFG-ETSIIT},
	abstract = {Plantilla de repositorio para trabajos fin de grado en la ETSIIT-UGR},
	urldate = {2024-12-15},
	author = {Guervós, Juan Julián Merelo},
	month = sep,
	year = {2024},
	note = {original-date: 2019-03-19T11:58:46Z},
	keywords = {hacktoberfest, hacktoberfest2021},
}

@article{redheffer_machine_1948,
	title = {A {Machine} for {Playing} the {Game} {Nim}},
	volume = {55},
	issn = {0002-9890},
	url = {https://doi.org/10.1080/00029890.1948.11999249},
	doi = {10.1080/00029890.1948.11999249},
	number = {6},
	urldate = {2025-06-06},
	journal = {The American Mathematical Monthly},
	author = {Redheffer, Raymond},
	month = jun,
	year = {1948},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00029890.1948.11999249},
	pages = {343--349},
}

@misc{openai_dota_2019,
	title = {Dota 2 with {Large} {Scale} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1912.06680},
	doi = {10.48550/arXiv.1912.06680},
	abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {OpenAI and Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dębiak, Przemysław and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and Józefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Pinto, Henrique P. d O. and Raiman, Jonathan and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
	month = dec,
	year = {2019},
	note = {arXiv:1912.06680 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{russell_artificial_2020,
	address = {Hoboken},
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}},
	isbn = {978-0-13-461099-3},
	shorttitle = {Artificial {Intelligence}},
	abstract = {The most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence The long-anticipated revision of Artificial Intelligence: A Modern Approach explores the full breadth and depth of the field of artificial intelligence (AI). The 4th Edition brings readers up to date on the latest technologies, presents concepts in a more unified manner, and offers new or expanded coverage of machine learning, deep learning, transfer learning, multiagent systems, robotics, natural language processing, causality, probabilistic programming, privacy, fairness, and safe AI.},
	language = {Inglés},
	author = {Russell, Stuart and Norvig, Peter},
	month = apr,
	year = {2020},
}

@misc{ghasemi_comprehensive_2025,
	title = {Comprehensive {Survey} of {Reinforcement} {Learning}: {From} {Algorithms} to {Practical} {Challenges}},
	url = {https://arxiv.org/html/2411.18892v2#bib.bib84},
	urldate = {2025-06-12},
	author = {Ghasemi, Majid and Hossein, Amir and Ebrahimi, Dariush},
	month = feb,
	year = {2025},
	file = {Comprehensive Survey of Reinforcement Learning\: From Algorithms to Practical Challenges:C\:\\Users\\silve\\Zotero\\storage\\YKASE857\\2411.html:text/html},
}

@inproceedings{abed_steering_2020,
	title = {Steering {Control} for {Autonomous} {Vehicles} {Using} {PID} {Control} with {Gradient} {Descent} {Tuning} and {Behavioral} {Cloning} {\textbar} {Request} {PDF}},
	url = {https://www.researchgate.net/publication/347043752_Steering_Control_for_Autonomous_Vehicles_Using_PID_Control_with_Gradient_Descent_Tuning_and_Behavioral_Cloning},
	doi = {10.1109/NILES50944.2020.9257946},
	abstract = {Request PDF {\textbar} On Oct 24, 2020, Mohamed Esmail Abed and others published Steering Control for Autonomous Vehicles Using PID Control with Gradient Descent Tuning and Behavioral Cloning {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-06-12},
	booktitle = {{ResearchGate}},
	author = {Abed, Mohamed and Mo'men, Aly and Hossam, Hassan and Elsayed, Raafat},
	month = oct,
	year = {2020},
}

@misc{gaudreau_game_2025,
	title = {Game {AI} {Summit}: {No} {Brakes}! {Machine} {Learning} {Vehicles} in '{Star} {Wars} {Outlaws}'},
	shorttitle = {Game {AI} {Summit}},
	url = {https://gdcvault.com/play/1035556/Game-AI-Summit-No-Brakes},
	abstract = {In this session, Colin and Andreas present their experience making the machine learning (ML)-based vehicle AI system in Star Wars Outlaws. They discuss why they chose an ML-based solution and the advantages it provides over alternatives....},
	urldate = {2025-06-12},
	author = {Gaudreau, Colin and Lasses, Andreas},
	year = {2025},
}

@misc{epic_games_ml_2025,
	title = {{ML} {Deformer} {Framework} in {Unreal} {Engine} {\textbar} {Unreal} {Engine} 5.6 {Documentation} {\textbar} {Epic} {Developer} {Community}},
	url = {https://dev.epicgames.com/documentation/en-us/unreal-engine/ml-deformer-framework-in-unreal-engine},
	abstract = {Use the Machine Learning ML Deformer Framework to train ML Deformer Models to make high quality character skeletal mesh deformation selections at runtim...},
	language = {en-us},
	urldate = {2025-06-12},
	journal = {Epic Games Developer},
	author = {Epic Games},
	year = {2025},
}

@misc{epic_games_behavior_2024,
	title = {Behavior {Tree} in {Unreal} {Engine} - {Overview} {\textbar} {Unreal} {Engine} 5.6 {Documentation} {\textbar} {Epic} {Developer} {Community}},
	url = {https://dev.epicgames.com/documentation/en-us/unreal-engine/behavior-tree-in-unreal-engine---overview},
	abstract = {Describes the concepts behind Behavior Trees in Unreal Engine and how they differ from traditional Behavior Trees.},
	language = {en-us},
	urldate = {2025-06-12},
	journal = {Epic Games Developer},
	author = {Epic Games},
	year = {2024},
}

@misc{cd_projekt_red_witcher_2025,
	title = {The {Witcher} 4 {Unreal} {Engine} 5 {Tech} {Demo} {4K} {\textbar} {State} of {Unreal} {\textbar} {Unreal} {Fest} {Orlando}},
	url = {https://www.youtube.com/watch?v=aorRfK478RE},
	urldate = {2025-06-12},
	author = {{CD Projekt Red}},
	month = jun,
	year = {2025},
}

@misc{tommy_thompson_building_2025,
	title = {Building {AI} {Speeders} with {Machine} {Learning} in {Star} {Wars} {Outlaws} {\textbar} {AI} and {Games} \#80},
	url = {https://www.youtube.com/watch?v=I7U1xleVh_g},
	urldate = {2025-06-12},
	author = {{Tommy Thompson}},
	month = may,
	year = {2025},
}

@misc{jeff_gdc_2006,
	title = {{GDC} {Vault} - {Three} {States} and a {Plan}: {The} {AI} of {F}.{E}.{A}.{R}.},
	url = {https://gdcvault.com/play/1013282/Three-States-and-a-Plan},
	urldate = {2025-06-12},
	author = {Jeff, Orkin},
	year = {2006},
	file = {GDC Vault - Three States and a Plan\: The AI of F.E.A.R.:C\:\\Users\\silve\\Zotero\\storage\\U8NQLBJ8\\Three-States-and-a-Plan.html:text/html},
}

@misc{isla_managing_2005,
	title = {Managing {Complexity} in the {Halo} 2 {AI} {System}},
	url = {https://www.gdcvault.com/play/1020270/Managing-Complexity-in-the-Halo},
	urldate = {2025-06-12},
	author = {Isla, Damian},
	year = {2005},
}

@misc{universidad_europea_que_2025,
	title = {¿{Qué} es mesh en el desarrollo de videojuegos? {\textbar} {Blog} {CC}},
	shorttitle = {¿{Qué} es mesh en el desarrollo de videojuegos?},
	url = {https://creativecampus.universidadeuropea.com/blog/meshes/},
	abstract = {Los meshes son estructuras 3D que definen todos los elementos visuales en videojuegos, desde personajes hasta escenarios.},
	language = {es},
	urldate = {2025-06-12},
	journal = {Universidad Europea Creative Campus},
	author = {Universidad Europea},
	month = apr,
	year = {2025},
	note = {Section: Blog},
}

@misc{mike_game_2016,
	title = {Game {Programming} {Concepts}–{Finite} {State} {Machines}},
	url = {https://gamefromscratch.com/game-programming-concepts-finite-state-machines/},
	abstract = {Welcome to a new tutorial series here on GameFromScratch.com where we will be looking at core concepts of game programming.  This includes data structures, design patterns and algorithms commonly used in game development.  We start the series off with Finite State Machines.   There is an HD video version of this tutorial available here [...]},
	language = {en-US},
	urldate = {2025-06-12},
	journal = {GameFromScratch.com},
	author = {Mike},
	month = jun,
	year = {2016},
}

@misc{wikipedia_artificial_2025,
	title = {Artificial intelligence in video games},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_intelligence_in_video_games&oldid=1292216335},
	abstract = {In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in 1948, first seen in the game Nim. AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player's input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation. One of the most infamous examples of this NPC technology and gradual difficulty levels can be found in the game Mike Tyson's Punch-Out!! (1987).  
In general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence.},
	language = {en},
	urldate = {2025-06-12},
	journal = {Wikipedia},
	author = {Wikipedia},
	month = may,
	year = {2025},
	note = {Page Version ID: 1292216335},
}

@misc{wikipedia_diseno_2025,
	title = {Diseño de juegos},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://es.wikipedia.org/w/index.php?title=Dise%C3%B1o_de_juegos&oldid=167396849},
	abstract = {El diseño de juegos es el proceso de diseñar el contenido, los antecedentes y las reglas de un juego y de cualquier sistema interactivo recreativo. Los diseñadores de juegos profesionales se especializan en ciertos tipos de juegos, como juegos de mesa, juegos de naipes o videojuegos, al igual que en ciertas áreas que requieran la interacción entre una persona y un sistema. Ayudan a facilitar la interacción entre jugadores, ya sea con fines médicos, militares, educacionales o por entretenimiento. El diseño de juegos suele ser aplicado principalmente a los ambientes virtuales, aunque se sigue empleando para elaborar juegos fuera del espacio virtual, además de ser empleado para crear sistemas interactivos afuera del área de juegos.[1]​},
	language = {es},
	urldate = {2025-06-12},
	journal = {Wikipedia, la enciclopedia libre},
	author = {Wikipedia},
	month = may,
	year = {2025},
	note = {Page Version ID: 167396849},
}

@article{torres-jimenez_applications_2014,
	title = {Applications of metaheuristics in real-life problems},
	volume = {2},
	issn = {2192-6360},
	url = {https://doi.org/10.1007/s13748-014-0051-8},
	doi = {10.1007/s13748-014-0051-8},
	language = {en},
	number = {4},
	urldate = {2025-06-12},
	journal = {Progress in Artificial Intelligence},
	author = {Torres-Jiménez, Jose and Pavón, Juan},
	month = jul,
	year = {2014},
	pages = {175--176},
}

@article{wolpert_no_1997,
	title = {No free lunch theorems for optimization},
	volume = {1},
	issn = {1941-0026},
	url = {https://ieeexplore.ieee.org/document/585893},
	doi = {10.1109/4235.585893},
	abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
	number = {1},
	urldate = {2025-06-12},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Wolpert, D.H. and Macready, W.G.},
	month = apr,
	year = {1997},
	keywords = {Algorithm design and analysis, Bayesian methods, Evolutionary computation, Information theory, Iron, Minimax techniques, Performance analysis, Probability distribution, Simulated annealing},
	pages = {67--82},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2025-06-14},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Statistics},
	pages = {350--354},
}

@article{wydmuch_vizdoom_2019,
	title = {{ViZDoom} {Competitions}: {Playing} {Doom} from {Pixels}},
	volume = {11},
	issn = {2475-1502, 2475-1510},
	shorttitle = {{ViZDoom} {Competitions}},
	url = {http://arxiv.org/abs/1809.03470},
	doi = {10.1109/TG.2018.2877047},
	abstract = {This paper presents the first two editions of Visual Doom AI Competition, held in 2016 and 2017. The challenge was to create bots that compete in a multi-player deathmatch in a first-person shooter (FPS) game, Doom. The bots had to make their decisions based solely on visual information, i.e., a raw screen buffer. To play well, the bots needed to understand their surroundings, navigate, explore, and handle the opponents at the same time. These aspects, together with the competitive multi-agent aspect of the game, make the competition a unique platform for evaluating the state of the art reinforcement learning algorithms. The paper discusses the rules, solutions, results, and statistics that give insight into the agents' behaviors. Best-performing agents are described in more detail. The results of the competition lead to the conclusion that, although reinforcement learning can produce capable Doom bots, they still are not yet able to successfully compete against humans in this game. The paper also revisits the ViZDoom environment, which is a flexible, easy to use, and efficient 3D platform for research for vision-based reinforcement learning, based on a well-recognized first-person perspective game Doom.},
	number = {3},
	urldate = {2025-06-14},
	journal = {IEEE Transactions on Games},
	author = {Wydmuch, Marek and Kempka, Michał and Jaśkowski, Wojciech},
	month = sep,
	year = {2019},
	note = {arXiv:1809.03470 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {248--259},
}

@misc{vitcon_thuongmhhstreet-fighter-ai_2024,
	title = {thuongmhh/{Street}-{Fighter}-{AI}},
	url = {https://github.com/thuongmhh/Street-Fighter-AI},
	abstract = {Trained with Reinforcement Learning / PPO / PyTorch / Stable Baselines},
	urldate = {2025-06-14},
	author = {VitCon},
	month = apr,
	year = {2024},
	note = {original-date: 2022-02-12T10:05:19Z},
}

@misc{moschopoulos_lucy-skg_2023,
	title = {Lucy-{SKG}: {Learning} to {Play} {Rocket} {League} {Efficiently} {Using} {Deep} {Reinforcement} {Learning}},
	shorttitle = {Lucy-{SKG}},
	url = {http://arxiv.org/abs/2305.15801},
	doi = {10.48550/arXiv.2305.15801},
	abstract = {A successful tactic that is followed by the scientific community for advancing AI is to treat games as problems, which has been proven to lead to various breakthroughs. We adapt this strategy in order to study Rocket League, a widely popular but rather under-explored 3D multiplayer video game with a distinct physics engine and complex dynamics that pose a significant challenge in developing efficient and high-performance game-playing agents. In this paper, we present Lucy-SKG, a Reinforcement Learning-based model that learned how to play Rocket League in a sample-efficient manner, outperforming by a notable margin the two highest-ranking bots in this game, namely Necto (2022 bot champion) and its successor Nexto, thus becoming a state-of-the-art agent. Our contributions include: a) the development of a reward analysis and visualization library, b) novel parameterizable reward shape functions that capture the utility of complex reward types via our proposed Kinesthetic Reward Combination (KRC) technique, and c) design of auxiliary neural architectures for training on reward prediction and state representation tasks in an on-policy fashion for enhanced efficiency in learning speed and performance. By performing thorough ablation studies for each component of Lucy-SKG, we showed their independent effectiveness in overall performance. In doing so, we demonstrate the prospects and challenges of using sample-efficient Reinforcement Learning techniques for controlling complex dynamical systems under competitive team-based multiplayer conditions.},
	urldate = {2025-06-14},
	publisher = {arXiv},
	author = {Moschopoulos, Vasileios and Kyriakidis, Pantelis and Lazaridis, Aristotelis and Vlahavas, Ioannis},
	month = may,
	year = {2023},
	note = {arXiv:2305.15801 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 24 pages, 11 figures},
}

@misc{ai_and_games_why_2024,
	title = {Why is {It} {Difficult} to {Make} {Good} {AI} for {Games}? {\textbar} {AI} 101},
	shorttitle = {Why is {It} {Difficult} to {Make} {Good} {AI} for {Games}?},
	url = {https://www.youtube.com/watch?v=qCkqpRnk1oU},
	abstract = {Having made over 100 videos digging into how AI works in video games, I've never really explained why this is such a big problem.  Let's discuss some core AI fundamentals.},
	urldate = {2025-06-14},
	author = {{AI and Games}},
	month = jan,
	year = {2024},
}

@article{wurman_outracing_2022,
	title = {Outracing champion {Gran} {Turismo} drivers with deep reinforcement learning},
	volume = {602},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04357-7},
	doi = {10.1038/s41586-021-04357-7},
	abstract = {Many potential applications of artificial intelligence involve making real-time decisions in physical systems while interacting with humans. Automobile racing represents an extreme example of these conditions; drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1. Racing simulations, such as the PlayStation game Gran Turismo, faithfully reproduce the non-linear control challenges of real race cars while also encapsulating the complex multi-agent interactions. Here we describe how we trained agents for Gran Turismo that can compete with the world’s best e-sports drivers. We combine state-of-the-art, model-free, deep reinforcement learning algorithms with mixed-scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics. In addition, we construct a reward function that enables the agent to be competitive while adhering to racing’s important, but under-specified, sportsmanship rules. We demonstrate the capabilities of our agent, Gran Turismo Sophy, by winning a head-to-head competition against four of the world’s best Gran Turismo drivers. By describing how we trained championship-level racers, we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms.},
	language = {en},
	number = {7896},
	urldate = {2025-06-14},
	journal = {Nature},
	author = {Wurman, Peter R. and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J. and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and Gilpin, Leilani and Khandelwal, Piyush and Kompella, Varun and Lin, HaoChih and MacAlpine, Patrick and Oller, Declan and Seno, Takuma and Sherstan, Craig and Thomure, Michael D. and Aghabozorgi, Houmehr and Barrett, Leon and Douglas, Rory and Whitehead, Dion and Dürr, Peter and Stone, Peter and Spranger, Michael and Kitano, Hiroaki},
	month = feb,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Applied mathematics},
	pages = {223--228},
}

@article{togelius_search-based_2011,
	title = {Search-{Based} {Procedural} {Content} {Generation}: {A} {Taxonomy} and {Survey}},
	volume = {3},
	issn = {1943-0698},
	shorttitle = {Search-{Based} {Procedural} {Content} {Generation}},
	url = {https://ieeexplore.ieee.org/document/5756645},
	doi = {10.1109/TCIAIG.2011.2148116},
	abstract = {The focus of this survey is on research in applying evolutionary and other metaheuristic search algorithms to automatically generating content for games, both digital and nondigital (such as board games). The term search-based procedural content generation is proposed as the name for this emerging field, which at present is growing quickly. A taxonomy for procedural content generation is devised, centering on what kind of content is generated, how the content is represented and how the quality/fitness of the content is evaluated; search-based procedural content generation in particular is situated within this taxonomy. This article also contains a survey of all published papers known to the authors in which game content is generated through search or optimisation, and ends with an overview of important open research problems.},
	number = {3},
	urldate = {2025-06-14},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Togelius, Julian and Yannakakis, Georgios N. and Stanley, Kenneth O. and Browne, Cameron},
	month = sep,
	year = {2011},
	keywords = {Games, Algorithm design and analysis, Evolutionary computation, Buildings, Computer graphics, design automation, Encoding, evolutionary computation, genetic algorithms, Optimization, Weapons},
	pages = {172--186},
}

@article{geijtenbeek_flexible_2013,
	title = {Flexible muscle-based locomotion for bipedal creatures},
	volume = {32},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2508363.2508399},
	doi = {10.1145/2508363.2508399},
	abstract = {We present a muscle-based control method for simulated bipeds in which both the muscle routing and control parameters are optimized. This yields a generic locomotion control method that supports a variety of bipedal creatures. All actuation forces are the result of 3D simulated muscles, and a model of neural delay is included for all feedback paths. As a result, our controllers generate torque patterns that incorporate biomechanical constraints. The synthesized controllers find different gaits based on target speed, can cope with uneven terrain and external perturbations, and can steer to target directions.},
	number = {6},
	urldate = {2025-06-14},
	journal = {ACM Trans. Graph.},
	author = {Geijtenbeek, Thomas and van de Panne, Michiel and van der Stappen, A. Frank},
	month = nov,
	year = {2013},
	pages = {206:1--206:11},
}

@article{snell_evolutionary_2021,
	title = {An evolutionary approach to balancing and disrupting real-time strategy games},
	url = {https://ro.ecu.edu.au/ecuworkspost2013/11752},
	doi = {10.36334/modsim.2021.M8.snell},
	journal = {MODSIM2021, 24th International Congress on Modelling and Simulation},
	author = {Snell, Jacob and Masek, Martin and Lam, Chiou},
	month = jan,
	year = {2021},
	file = {text/html Attachment:C\:\\Users\\silve\\Zotero\\storage\\DGV8CBJK\\11752.html:text/html},
}

@article{garcia-sanchez_optimizing_2020,
	title = {Optimizing {Hearthstone} agents using an evolutionary algorithm},
	volume = {188},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705119304356},
	doi = {10.1016/j.knosys.2019.105032},
	abstract = {Digital collectible card games are not only a growing part of the video game industry, but also an interesting research area for the field of computational intelligence. This game genre allows researchers to deal with hidden information, uncertainty and planning, among other aspects. This paper proposes the use of evolutionary algorithms (EAs) to develop agents who play a card game, Hearthstone, by optimizing a data-driven decision-making mechanism that takes into account all the elements currently in play. Agents feature self-learning by means of a competitive coevolutionary training approach, whereby no external sparring element defined by the user is required for the optimization process. One of the agents developed through the proposed approach was runner-up (best 6\%) in an international Hearthstone Artificial Intelligence (AI) competition. Our proposal performed remarkably well, even when it faced state-of-the-art techniques that attempted to take into account future game states, such as Monte-Carlo Tree search. This outcome shows how evolutionary computation could represent a considerable advantage in developing AIs for collectible card games such as Hearthstone.},
	urldate = {2025-06-14},
	journal = {Knowledge-Based Systems},
	author = {García-Sánchez, Pablo and Tonda, Alberto and Fernández-Leiva, Antonio J. and Cotta, Carlos},
	month = jan,
	year = {2020},
	keywords = {Artificial intelligence, Games, Card games, Collectible card games, Evolution strategy, Evolutionary algorithms, Hearthstone, Videogames},
	pages = {105032},
}

@misc{adam_ciezkowski_developing_2023,
	title = {Developing {Card} {Playing} {Agent} for  {Tales} of {Tribute} {AI} {Competition}},
	url = {https://jakubkowalski.tech/Supervising/Ciezkowski2023DevelopingCard.pdf},
	author = {{Adam Ciężkowski} and {Artur Krzyżyński}},
	year = {2023},
}

@misc{magi-soft_development_magic_2002,
	title = {Magic {Workstation}, {Cards} {Management} and {Decks} {Testing} for {CCG} {Players}, {Online} {Play}},
	url = {https://www.magicworkstation.com/},
	urldate = {2025-06-14},
	author = {{Magi-Soft Development}},
	year = {2002},
	file = {Magic Workstation, Cards Management and Decks Testing for CCG Players, Online Play:C\:\\Users\\silve\\Zotero\\storage\\67CVWLZE\\www.magicworkstation.com.html:text/html},
}

@misc{hearthsim_hearthsimsabberstone_2017,
	title = {{HearthSim}/{SabberStone}},
	copyright = {AGPL-3.0},
	url = {https://github.com/HearthSim/SabberStone},
	abstract = {Just another Hearthstone Simulator in C\# .Net Core, with some A.I. approaches!},
	urldate = {2025-06-14},
	publisher = {HearthSim},
	author = {{HearthSim}},
	month = jan,
	year = {2017},
	note = {original-date: 2017-01-17T08:58:37Z},
	keywords = {hearthstone, hearthstone-simulator},
}

@misc{ematerasu_scriptsoftribute_2022,
	title = {{ScriptsOfTribute} · {GitHub}},
	url = {https://github.com/ScriptsOfTribute},
	urldate = {2025-06-14},
	author = {{Ematerasu} and {Jakub Kowalski} and {Radosław Miernik}},
	year = {2022},
	file = {ScriptsOfTribute · GitHub:C\:\\Users\\silve\\Zotero\\storage\\983G6K9H\\ScriptsOfTribute.html:text/html},
}

@misc{simao_reis_vgc_2019,
	title = {{VGC} {AI} {Framework}},
	url = {https://gitlab.com/DracoStriker/pokemon-vgc-engine},
	abstract = {The VGC AI Framework aims to emulate the Esports scenario of human video game championships of Pokémon with AI agents, including the game balance aspect.},
	language = {en},
	urldate = {2025-06-14},
	journal = {GitLab},
	author = {{Simão Reis}},
	month = oct,
	year = {2019},
	file = {Snapshot:C\:\\Users\\silve\\Zotero\\storage\\6DYENG4Q\\pokemon-vgc-engine.html:text/html},
}

@misc{thompson_how_2025,
	title = {How {AI} is {Actually} {Used} in the {Video} {Games} {Industry}},
	url = {https://www.aiandgames.com/p/how-ai-is-actually-used-in-the-video},
	abstract = {An Overview of the State of the Industry in 2024},
	language = {en},
	urldate = {2025-06-15},
	author = {Thompson, Tommy},
	month = apr,
	year = {2025},
	file = {Snapshot:C\:\\Users\\silve\\Zotero\\storage\\4Y8AMMML\\how-ai-is-actually-used-in-the-video.html:text/html},
}

@misc{thompson_ai_2024,
	title = {{AI} 101: {Introducing} {Utility} {AI}},
	shorttitle = {{AI} 101},
	url = {https://www.aiandgames.com/p/ai-101-introducing-utility-ai},
	abstract = {Understanding the value of action in context.},
	language = {en},
	urldate = {2025-06-15},
	author = {Thompson, Tommy},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\silve\\Zotero\\storage\\SBMTIEU2\\ai-101-introducing-utility-ai.html:text/html},
}

@misc{swiechowski_improving_2018,
	title = {Improving {Hearthstone} {AI} by {Combining} {MCTS} and {Supervised} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1808.04794},
	doi = {10.48550/arXiv.1808.04794},
	abstract = {We investigate the impact of supervised prediction models on the strength and efficiency of artificial agents that use the Monte-Carlo Tree Search (MCTS) algorithm to play a popular video game Hearthstone: Heroes of Warcraft. We overview our custom implementation of the MCTS that is well-suited for games with partially hidden information and random effects. We also describe experiments which we designed to quantify the performance of our Hearthstone agent's decision making. We show that even simple neural networks can be trained and successfully used for the evaluation of game states. Moreover, we demonstrate that by providing a guidance to the game state search heuristic, it is possible to substantially improve the win rate, and at the same time reduce the required computations.},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Świechowski, Maciej and Tajmajer, Tomasz and Janusz, Andrzej},
	month = aug,
	year = {2018},
	note = {arXiv:1808.04794 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Proceedings of the 2018 IEEE Conference on Computational Intelligence and Games (CIG'18); pages 445-452; ISBN: 978-1-5386-4358-7},
}

@misc{silver_mastering_2017,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	doi = {10.48550/arXiv.1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv:1712.01815 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

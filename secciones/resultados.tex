\chapter{Resultados y discusión} \label{chap:resultados}
% Interpretar los resultados de cómo y por qué
% - ¿Por qué el modo híbrido funciona mejor que los otros?
% - La tendencia de los pesos a irse a 0 o 1
% - Limitaciones del enfoque: bot sin memoria ni planificación, etc

\section{Análisis de las configuraciones híbridas} \label{sec:analisis_configuraciones_hibridas}

\section{Análisis de los modos de entrenamiento} \label{sec:analisis_modos_entrenamiento}

\section{Análisis del salón de la fama} \label{sec:analisis_salon_fama}


% Decir que se han cumplido los dos objetivos faltantes:
% OG5: Evaluar cuantitativamente el rendimiento del agente entrenado mediante las diferentes estrategias, utilizando métricas relevantes.
% OG6: Analizar comparativamente la efectividad y eficiencia de las estrategias de optimización implementadas, discutiendo sus ventajas y desventajas en el contexto específico de ``Scripts of Tribute''.



% https://en.wikipedia.org/wiki/AlphaStar_(software)
% https://www.bbc.com/news/technology-50212841
% Unlike AlphaZero, AlphaStar initially learns to imitate the moves of the best players in its database of human vs. human games; this step is necessary to solve what DeepMind's Dave Silver calls "the exploration problem": discovering new strategies would otherwise be like finding a "needle in a haystack". Agents then play each other and deploy deep reinforcement learning. These main agents also learn by playing against suboptimal "exploiter agents" whose purpose is to expose weaknesses in the main agents.
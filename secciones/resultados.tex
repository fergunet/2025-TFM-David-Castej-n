\chapter{Resultados y discusión} \label{chap:resultados}
% Interpretar los resultados de cómo y por qué
% - ¿Por qué el modo híbrido funciona mejor que los otros?
% - La tendencia de los pesos a irse a 0 o 1
% - Limitaciones del enfoque: bot sin memoria ni planificación, etc

\section{A nivel de población} \label{sec:a_nivel_de_poblacion}


\subsection{Evolución del fitness} \label{sec:evolucion_fitness_poblacion}


\subsection{Pesos medios} \label{sec:pesos_medios_poblacion}


\subsection{Pesos a lo largo de las generaciones} \label{sec:pesos_a_lo_largo_generaciones_poblacion}


\subsection{Variación de los pesos finales} \label{sec:variacion_pesos_finales_poblacion}


\section{A nivel de líderes} \label{sec:a_nivel_de_lideres}


\subsection{Evolución del fitness} \label{sec:evolucion_fitness_lideres}


\subsection{Pesos medios} \label{sec:pesos_medios_lideres}


\subsection{Pesos a lo largo de las generaciones} \label{sec:pesos_a_lo_largo_generaciones_lideres}


\subsection{Variación de los pesos finales} \label{sec:variacion_pesos_finales_lideres}





% https://en.wikipedia.org/wiki/AlphaStar_(software)
% https://www.bbc.com/news/technology-50212841
% Unlike AlphaZero, AlphaStar initially learns to imitate the moves of the best players in its database of human vs. human games; this step is necessary to solve what DeepMind's Dave Silver calls "the exploration problem": discovering new strategies would otherwise be like finding a "needle in a haystack". Agents then play each other and deploy deep reinforcement learning. These main agents also learn by playing against suboptimal "exploiter agents" whose purpose is to expose weaknesses in the main agents.
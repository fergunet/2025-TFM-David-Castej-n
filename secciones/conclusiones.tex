\chapter{Conclusiones} \label{chap:conclusiones}

En este proyecto de Fin de Máster se ha demostrado cómo diferentes enfoques de una estrategia evolutiva pueden ser implementados con éxito en un entorno complejo como \textit{Scripts of Tribute}. Partiendo de la metodología creada anteriormente para \textit{Hearthstone}, se ha adaptado y expandido el planteamiento del algoritmo, demostrando una vez más la versatilidad de los métodos evolutivos. Además de incorporar la metodología de entrenamiento coevolutivo, se añadieron dos modos nuevos de entrenamiento: el fijo y el híbrido. El primero permite una gran especialización de combate contra los bots que se usan durante el proceso evolutivo, mientras que el segundo combina las ventajas de ambos modos anteriores, permitiendo una mayor exploración de estrategias y tácticas. Si bien el modo híbrido no ha demostrado ser superior a los otros dos modos en torneos específicos que repiten las condiciones de entrenamiento, sí que ha demostrado ser una buena opción para explorar nuevas estrategias y podría ser la opción más adecuada contra oponentes desconocidos.

Además de los nuevos modos de entrenamiento, se ha implementado un sistema de salón de la fama basado en el trabajo de Nogueira et al. Este sistema permite guardar los mejores individuos de cada generación y utilizarlos como oponentes en futuras generaciones, lo que puede mejorar el rendimiento del agente en entornos de entrenamiento estables. Los torneos finales demuestran que el salón de la fama tienen un efecto positivo en los modos de entrenamiento fijo y coevolutivo, generando bots con un rendimiento superior al de los que no lo usaron. Sin embargo, en el modo híbrido el salón de la fama no aportó beneficios y, en algunos casos, incluso perjudicó al rendimiento del agente. Esto sugiere que pese a ser una herramienta útil, la efectividad del salón de la fama depende del objetivo actual de la población. Estos resultados, donde ningún modo de entrenamiento demuestra ser universalmente superior, sino que cada uno destaca en un contexto de evaluación específico, son una clara manifestación práctica del teorema ``No Free Lunch''. La estrategia de entrenamiento de un agente debe estar íntimamente ligada al entorno y a los oponentes que se espera que enfrente, y no existe una única estrategia que funcione como la mejor en todos los casos.

Todos los objetivos del proyecto se han cumplido, marcándose a lo largo de la memoria al final de los capítulos \ref{chap:antecedentes}, \ref{chap:plataforma_trabajo}, 
\ref{chap:software_bot}, \ref{chap:entrenador}, \ref{chap:experimentacion} y \ref{chap:resultados}.

\section{Líneas de trabajo futuro} \label{sec:trabajo_futuro}

La unión de heurísticas específicas junto con los pesos generados por el algoritmo evolutivo han permitido crear un bot con rendimiento altamente escalable, al menos contra los agentes autónomos de \textit{Scripts of Tribute}. Sin embargo, aun existen muchas áreas de mejora que se pueden explorar en el futuro. Por ejemplo, endurecer esos mismos oponentes durante el entrenamiento podría desembocar en pesos que produzcan dinámicas más capaces de adaptarse a diferentes estilos de juego. El camino más claro para obtener mejores oponentes es conseguir que los participantes de las competiciones de otros años funcionen correctamente en la versión actual del motor. Si bien no sería un proceso sencillo dado que cada uno de esos bots son más complejos que los que SoT proporciona por defecto, la recompensa podría ser un gran salto cualitativo en el rendimiento del agente. Un aspecto secundario positivo de este enfoque es que se podrían revisar las heurísticas que dichos bots utilizan, las cuales podrían reimplementarse en el EvolutionaryBot y así incluir más conocimiento experto en su comportamiento.

Aunque en este trabajo se ha puesto especial atención a los oponentes que se enfrentan al agente en los enfrentamientos, así como el orden y la cantidad de partidas que estos juegan, no se ha explorado en profundidad el impacto de alterar los algoritmos base que conforman la estrategia evolutiva. En ese sentido, se podría experimentar con operadores de mutación y cruce más sofisticados de entre los disponibles en la librería \textit{inspyred}, llegando incluso a implementar un ajuste de hiperparámetros para encontrar aquellos que maximicen la exploración y la explotación en las diferentes etapas del entrenamiento, de una forma análoga al modo híbrido de entrenamiento.

Otra forma de mejorar el rendimiento del agente podría ser determinando los mazos más efectivos a ser jugados por el agente. Durante todo este proyecto se ha utilizado un selector aleatorio de mazos, lo cual es lo más indicado para mejorar la exploración durante el entrenamiento; pero una vez se tiene el conjunto de pesos final, sería un buen momento para afinar la potencia existente lo máximo posible. Para ello, simplemente se tendrían que analizar un gran número de partidas y ver cuáles son los mazos que más victorias consiguen. Con esta información, se podría implementar un selector más sofisticado que elija el mazo con la mayor probabilidad de ganar en cada partida.

Por último, un cambio a un nivel más holístico sería la refactorización del agente y el entrenador en un único lenguaje de programación. Aunque la idea de utilizar variables de entorno y lecturas de la salida estándar es una técnica perfectamente válida para la versión actual del proyecto, es posible que la unificación de ambos componentes bajo Python permita mejorar el rendimiento del entrenamiento. Para ello, se podría utilizar la implementación del método de comunicación ``gRPC'' que se añadió a SoT recientemente, lo que permitiría que toda la lógica del agente y el entrenador se ejecutara bajo un único programa orquestador. Este enfoque dejaría al GameRunner únicamente como un componente de entrada y salida de datos mediante microservicios. Cambiar la arquitectura base permitiría que el entrenador y el agente compartieran objetos y memoria directamente, abriendo la puerta al uso de otras técnicas más avanzadas como el aprendizaje por refuerzo profundo, que requiere de una alta transferencia de datos del entorno entre el agente y el GameRunner.

\vspace{30pt}

El código fuente de este proyecto se encuentra en el repositorio de Github:

\begin{center}
\url{https://github.com/Silver812/TFM}
\end{center}